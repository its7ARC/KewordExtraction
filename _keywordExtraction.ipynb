{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Extraction¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing dataset¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Self-Organization of Associative Database and ...   \n",
       "1  A Mean Field Theory of Layer IV of Visual Cort...   \n",
       "2  Storing Covariance by the Associative Long-Ter...   \n",
       "3  Bayesian Query Construction for Neural Network...   \n",
       "4  Neural Network Ensembles, Cross Validation, an...   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('papers.csv')\n",
    "df.head()\n",
    "del df['event_type']\n",
    "del df['abstract']\n",
    "del df['pdf_name']\n",
    "del df['year']\n",
    "del df['id']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['paper_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = list(df['title'])\n",
    "text = list(df['paper_text'])\n",
    "title = title[:10]\n",
    "text = text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anshumanrajchauhan_1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self,text):\n",
    "        self.__text = text\n",
    "        \n",
    "        #stopwords\n",
    "        self.__sw = stopwords.words('english')\n",
    "        self.__sw.append('and')\n",
    "        self.__sw.append('using')\n",
    "    \n",
    "    def stopwordRemoval(self,sen,sw):\n",
    "        sen = [word.lower() for word in sen if (len(word) > 2 and word.lower() not in sw)]\n",
    "        return sen\n",
    "\n",
    "    def lemmatizeWords(self,text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "        pos_tagged_text = nltk.pos_tag(text.split())\n",
    "        return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "        \n",
    "    def computeAOF(self,count):\n",
    "        aof = []\n",
    "        for i in count:\n",
    "            nTokens = 0\n",
    "            freq = 0\n",
    "            for j in i:\n",
    "                if(j > 0):\n",
    "                    nTokens += 1\n",
    "            aof.append(np.sum(i)/nTokens)\n",
    "\n",
    "        return aof\n",
    "    \n",
    "    def vectorise(self):\n",
    "        text = self.__text\n",
    "        corpus = []\n",
    "        tokenizer = RegexpTokenizer('[a-zA-Z\\']+') \n",
    "        for sen in text:\n",
    "            sen = tokenizer.tokenize(sen)\n",
    "            sen = self.stopwordRemoval(sen,self.__sw)\n",
    "            sen = ' '.join(word for word in sen)\n",
    "            sen = self.lemmatizeWords(sen)\n",
    "            corpus.append(sen)\n",
    "        \n",
    "        cv = CountVectorizer()\n",
    "        vect = cv.fit_transform(corpus)\n",
    "        count = vect.toarray()\n",
    "        vocab = cv.vocabulary_\n",
    "        aof = self.computeAOF(count)\n",
    "        corpus_new = []\n",
    "        \n",
    "        tokenizer = RegexpTokenizer('[a-zA-Z]+') \n",
    "        dummy = []\n",
    "        i = 0\n",
    "        for sen in corpus:\n",
    "            sen = tokenizer.tokenize(sen)\n",
    "            sw_new = [key for (key,value) in vocab.items() if count[i][value] < aof[i]]\n",
    "            sen = self.stopwordRemoval(sen,sw_new)\n",
    "            dummy.append(sen)\n",
    "            sen = ' '.join(word for word in sen)\n",
    "            corpus_new.append(sen)\n",
    "            i += 1\n",
    "    \n",
    "        vectNew = cv.fit_transform(corpus_new)\n",
    "        count = vectNew.toarray()\n",
    "        vocab = cv.vocabulary_\n",
    "        invVocab = dict(zip(vocab.values(),vocab.keys()))\n",
    "\n",
    "        corpusVect = []\n",
    "        for sen in dummy:\n",
    "            corpusVect.append([vocab[word] for word in sen])\n",
    "\n",
    "        return cv,vectNew,count,vocab,invVocab,aof,corpusVect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Preprocess(text)\n",
    "cv,vect,count,vocab,invVocab,aof,corpusVect = p.vectorise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Formulation¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self,V):\n",
    "        self.V = V\n",
    "        self.nodeWts = np.zeros((V))\n",
    "        self.wt = np.zeros((V,V))\n",
    "        self.F = np.zeros((V))\n",
    "        self.L = np.zeros((V))\n",
    "        self.SC = np.zeros((V))\n",
    "        self.TF = np.zeros((V))\n",
    "        self.D = np.zeros((V))\n",
    "        self.Neigh = np.zeros((V))\n",
    "            \n",
    "    def addEdgeWt(self,a,b,weight):\n",
    "        wt = self.wt\n",
    "        wt[a][b] = weight\n",
    "        wt[b][a] = weight\n",
    "        self.wt = wt\n",
    "\n",
    "    \n",
    "    def computeEdgeWt(self,freqMat,a,b):\n",
    "        fa = np.sum(freqMat,axis = 0)[a]\n",
    "        fb = np.sum(freqMat,axis = 0)[b]\n",
    "        count = 0\n",
    "        for i in range(np.shape(freqMat)[0]):\n",
    "            if(freqMat[i][a] > 0 and freqMat[i][b] > 0):\n",
    "                count += 1\n",
    "        fab = count\n",
    "        weight = (fab)/(fa + fb - fab)\n",
    "        self.addEdgeWt(a,b,weight)\n",
    "    \n",
    "    def computeNodeWt(self,count,vocab,corpusVect):\n",
    "        F = self.F\n",
    "        L = self.L\n",
    "        SC = self.SC\n",
    "        TF = self.TF\n",
    "        wt = self.wt\n",
    "        D = self.D\n",
    "        Neigh = self.Neigh\n",
    "        \n",
    "        #first/last word\n",
    "        for vect in corpusVect:\n",
    "            F[vect[0]] += 1\n",
    "            L[vect[-1]] += 1\n",
    "        for key in vocab.values():\n",
    "            if(F[key] > 0):\n",
    "                F[key] = F[key]/np.sum(count,axis = 0)[key]\n",
    "            if(L[key] > 0):\n",
    "                L[key] = L[key]/np.sum(count,axis = 0)[key]\n",
    "        \n",
    "        #SC, TF\n",
    "        degree = []\n",
    "        for i in range(np.shape(wt)[0]):\n",
    "            outDegree = 0\n",
    "            for j in range(np.shape(wt)[1]):\n",
    "                if(j == i):\n",
    "                    continue\n",
    "                if(wt[i][j] > 0):\n",
    "                    SC[i] += wt[i][j]\n",
    "                    outDegree += 1\n",
    "            degree.append(outDegree)\n",
    "            SC[i] = SC[i]/outDegree\n",
    "             \n",
    "        centralNode = np.argmax(degree)\n",
    "        #TF\n",
    "        TF = np.sum(count,axis = 0)/np.shape(count)[0]\n",
    "        \n",
    "        #D\n",
    "        for i in range(np.shape(wt)[0]):\n",
    "            if(i == centralNode):\n",
    "                continue\n",
    "            D[i] = wt[i][centralNode]\n",
    "        \n",
    "        D = D/np.max(D)         #Normalisation\n",
    "        D[centralNode] = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.wt = wt\n",
    "        self.F = F\n",
    "        self.L = L\n",
    "        self.wt = wt\n",
    "        self.SC = SC\n",
    "        self.TF = TF\n",
    "        self.D = D\n",
    "    \n",
    "        \n",
    "        self.nodeWts = F + L + SC + TF + D\n",
    "        nodeWts = self.nodeWts\n",
    "        minWt = np.min(nodeWts)\n",
    "        maxWt = np.max(nodeWts)\n",
    "        for i in range(self.V):\n",
    "            nodeWts[i] = (nodeWts[i] - minWt)/(maxWt - minWt)\n",
    "        \n",
    "        self.nodeWts = nodeWts\n",
    "        return\n",
    "        \n",
    "    \n",
    "    def makeGraph(self,corpusVect,vocab,count):\n",
    "        for a in vocab.values():\n",
    "            for b in vocab.values():\n",
    "                if(a == b):\n",
    "                    self.wt[a][b] = 0\n",
    "                    continue\n",
    "                \n",
    "                self.computeEdgeWt(count,a,b)\n",
    "        \n",
    "        self.computeNodeWt(count,vocab,corpusVect)\n",
    "        \n",
    "        return self.nodeWts,self.wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph(len(vocab))\n",
    "nodeWts, edgeWts = G.makeGraph(corpusVect,vocab,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NE Rank¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNERank(nodeWt,edgeWt,d):\n",
    "    V = np.shape(nodeWt)[0]\n",
    "    R = np.zeros((V))\n",
    "    sigma2 = np.sum(edgeWt,axis = 1)\n",
    "    for itr in range(10):\n",
    "        for i in range(V):\n",
    "            sigma1 = 0\n",
    "            for j in range(V):\n",
    "                if(i == j):\n",
    "                    continue\n",
    "                sigma1 += (edgeWt[i][j]*R[j])/sigma2[j]\n",
    "            \n",
    "            dummy = R[i]\n",
    "            R[i] = (1-d)*nodeWt[i] + d*nodeWt[i]*sigma1;\n",
    "            \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = computeNERank(nodeWts,edgeWts,0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing Top Keywords¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network\n",
      "neural\n",
      "input\n",
      "use\n",
      "show\n",
      "error\n",
      "model\n",
      "set\n",
      "control\n",
      "figure\n",
      "weight\n",
      "time\n",
      "function\n",
      "plasma\n",
      "test\n",
      "data\n",
      "cell\n",
      "value\n",
      "parameter\n",
      "result\n"
     ]
    }
   ],
   "source": [
    "topKeywords = R.argsort()[-20:][::-1]\n",
    "\n",
    "for i in range(20):\n",
    "    print(invVocab[topKeywords[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
